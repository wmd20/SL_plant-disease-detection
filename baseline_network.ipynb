{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network||| Plant Disease Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Images\n",
    "\n",
    "#### 利用 OpenCV (cv2) 读入照片并存放在 numpy array 中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readFile(path,flag):\n",
    "    image_dir=sorted(os.listdir(path))\n",
    "    x=np.zeros((len(image_dir),128,128,3),dtype=np.uint8)\n",
    "    y=np.zeros((len(image_dir)),dtype=np.uint8)\n",
    "    for i,file in enumerate(image_dir):\n",
    "        img=cv2.imread(os.path.join(path,file))\n",
    "        x[i,:,:]=cv2.resize(img,(128,128))\n",
    "        if flag:\n",
    "            y[i]=int(file.split('_')[0]) # label 的映射需要重新定义\n",
    "    if flag:\n",
    "        return x,y\n",
    "    else:\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分别将 training set,validation set,testing set 用readFile函数读进来\n",
    "workSpace_dir='./food-11'\n",
    "print(\"Reading data...\")\n",
    "train_x,train_y=readFile(os.path.join(workSpace_dir,\"training set\"),True)\n",
    "print(\"Size of training data={}\".format(len(train_x)))\n",
    "val_x,val_y=readFile(os.path.join(workSpace_dir,\"validation set\"),True)\n",
    "print(\"Size of validation data={}\".format(len(val_x)))\n",
    "test_x=readFile(os.path.join(workSpace_dir,\"testing set\"),False)\n",
    "print(\"Size of testing data={}\".format(len(test_x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "在Pytorch中，我们可以利用torch.utils.data的Dataset及DataLoader来“包装”data,使后续的training 及testing更方便。\n",
    "Dataset需要overload两个函数：_len_及_getitem_; \n",
    "实际上我们并不会直接使用到这两个函数，但是使用DataLoader在enumerate Dataset时会使用到，没有这样做的话在程序运行阶段会报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training时做 data augmentation\n",
    "train_transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomHorizontalFlip(), # 水平翻转\n",
    "    transforms.RandomRotation(15), # 随机旋转\n",
    "    transforms.ToTensor(), # 将图片转成Tensor,并把数组normalize到[0,1]（data normalization）\n",
    "])\n",
    "\n",
    "# testing时不需要做data augmentation\n",
    "test_transform=transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self,x,y=None,transform=None):\n",
    "        self.x=x\n",
    "        # label is required to be a LongTensor\n",
    "        self.y=y\n",
    "        if y is not None:\n",
    "            self.y=torch.LongTensor(y)\n",
    "        self.transform=transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X=self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X=self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y=self.y[index]\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=128\n",
    "train_set=ImgDataset(train_x,train_y,train _transform)\n",
    "val_set=ImgDataset(val_x,val_y,test_transform)\n",
    "train_loader=DataLoader(train_set,batch_size=batch_size,shuffle=True)\n",
    "val_loader=DataLoader(val_set,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Classifier,self).__init__()\n",
    "        # input dimension [3,128,128]\n",
    "        self.cnn=nn.Sequential(\n",
    "            nn.Conv2d(3,64,3,1,1),# [64,128,128] 对高度和宽度都进行卷积\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),# [64,64,64]\n",
    "\n",
    "            nn.Conv2d(64,128,3,1,1),# [128,64,64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),# [128,32,32]\n",
    "\n",
    "            nn.Conv2d(128,256,3,1,1),# [256,32,32]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),# [256,16,16]\n",
    "\n",
    "            nn.Conv2d(256,512,3,1,1),# [512,16,16]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),# [512,8,8]\n",
    "\n",
    "            nn.Conv2d(512,512,3,1,1),# [512,8,8]\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2,2,0),# [512,4,4]\n",
    "        )\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(512*4*4,1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024,512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512,61)\n",
    "        )\n",
    "    def forward(self,x):\n",
    "        out=self.cnn(x)\n",
    "        out=out.view(out.size()[0],-1)\n",
    "        return self.fc(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training\n",
    "使用training set 训练，并使用validation set寻找最好的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Classifier().cuda()\n",
    "loss=nn.CrossEntropyLoss() # 因为是 classification task,所以loss使用crossEntropyLoss\n",
    "optimizer=torch.optim.Adam(model.parameters(),lr=0.001)\n",
    "num_epoch=30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time=time.time()\n",
    "    train_acc=0.0\n",
    "    train_loss=0.0\n",
    "    val_acc=0.0\n",
    "    val_loss=0.0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for i,data in enumerate(train_loader):\n",
    "        optimizer.zero_grad() # 用optimizer 将model参数的gradient归零\n",
    "        train_pred=model(data[0].cuda()) #利用model得到预测的概率分布，这边实际上就是去call model的froward 函数\n",
    "        batch_loss=loss(train_pred,data[1].cuda()) #计算loss 注意 prediction跟label必须同时待在CPU或是GPU上\n",
    "        batch_loss.backward() # 利用back probagation 算出每个参数的gradient\n",
    "        optimizer.step() #更新参数\n",
    "        \n",
    "        train_acc+=np.sum(np.argmax(train_pred.cpu().data.numpy(),axis=1)==data[1].numpy())\n",
    "        train_loss+=batch_loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad(): # torch.no_grad() 是一个上下文管理器，被该语句 wrap 起来的部分将不会track 梯度\n",
    "        for i,data in enumerate(val_loader):\n",
    "            val_pred=model(data[0].cuda())\n",
    "            batch_loss=loss(val_pred,data[1].cuda())\n",
    "            \n",
    "            val_acc+=np.sum(np.argmax(val_pred.cpu().data.numpy(),axis=1)==data[1].numpy)\n",
    "            val_loss+=batch_loss.item()\n",
    "        \n",
    "        #将结果 print 出来\n",
    "        print('[%03d/%03d] %2.2f sec(s) Trian Acc:%3.6f Loss:%3.6f | Val Acc:%3.6f Loss:%3.6f' %\\\n",
    "             (epoch+1,num_epoch,time.time()-epoch_start_time,\\\n",
    "             train_acc/train_set.__len__(),train_loss/train_set.__len__(),\\\n",
    "             val_acc/val_set.__len__(),val_loss/val_set.__len__()))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 得到好的参数之后，我们使用training set 和validation set共同训练（数据量变多，模型效果更好）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_val_x=np.concatenate((train_x,val_x),axis=0)\n",
    "train_val_y=np.concatenate((train_y,val_y),axis=0)\n",
    "train_val_set=ImgDataset(train_val_x,train_val_y,train_transform)\n",
    "train_val_loader=DataLoader(train_val_set,batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best=Classifier().cuda()\n",
    "loss=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model_best.parameters(),lr=0.001)\n",
    "num_epoch=30\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    epoch_start_time=time.time()\n",
    "    train_acc=0.0\n",
    "    train_loss=0.0\n",
    "    \n",
    "    model_best.train()\n",
    "    for i,data in enumerate(train_val_loader):\n",
    "        optimizer.zero_grad()\n",
    "        train_pred=model_best(data[0].cuda())\n",
    "        batch_loss=loss(train_pred,data[1].cuda())\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        train_acc+=np.sum(np.argmax(train_pred.cpu().data.numpy(),axis=1)==data[1].numpy())\n",
    "        train_loss+=batch_loss.item()\n",
    "    #将结果 print 出来\n",
    "    print('[%03d/%03d] %2.2f sec(s) Trian Acc:%3.6f Loss:%3.6f' %\\\n",
    "         (epoch+1,num_epoch,time.time()-epoch_start_time,\\\n",
    "         train_acc/train_set.__len__(),train_loss/train_set.__len__()))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing\n",
    "利用train好的模型进行prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set=ImgDataset(test_x,transform=test_transform)\n",
    "test_loader=DataLoader(test_set,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best.eval()\n",
    "prediction=[]\n",
    "with torch.no_grad():\n",
    "    for i,data in enumerate(test_loader):\n",
    "        test_pred=model_best(data.cuda())\n",
    "        test_label=np.argmax(test_pred.cpu().data.numpy(),axis=1)\n",
    "        for y in test_label:\n",
    "            prediction.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将结果写入csv\n",
    "with open(\"predict.csv\",'w') as f:\n",
    "    f.write('Id,Class\\n')\n",
    "    for i,y in enumerate(prediction):\n",
    "        f.write('{},{}\\n'.format(i,y))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
